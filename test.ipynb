{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e48b191-ca69-4a6a-9449-d5e56477e3b0",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-18T02:10:04.458103Z",
     "iopub.status.busy": "2024-07-18T02:10:04.457735Z",
     "iopub.status.idle": "2024-07-18T02:10:06.832252Z",
     "shell.execute_reply": "2024-07-18T02:10:06.831663Z",
     "shell.execute_reply.started": "2024-07-18T02:10:04.458084Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "\n",
    "def extract_tar_gz(tar_gz_path, output_path):\n",
    "    with tarfile.open(tar_gz_path, 'r:gz') as tar:\n",
    "        tar.extractall(path=output_path)\n",
    "\n",
    "# 使用函数\n",
    "tar_gz_file = 'bert_pretrain/bert-base-chinese.tar.gz'  # 替换为你的文件实际路径\n",
    "output_directory = 'bert_pretrain/bert-base-chinese/'     # 替换为你希望解压到的目录\n",
    "\n",
    "extract_tar_gz(tar_gz_file, output_directory)\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebed24b6-7e10-4b37-98ef-263c31b61e4b",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2024-07-18T02:12:02.337349Z",
     "iopub.status.busy": "2024-07-18T02:12:02.336982Z",
     "iopub.status.idle": "2024-07-18T02:12:02.356389Z",
     "shell.execute_reply": "2024-07-18T02:12:02.355846Z",
     "shell.execute_reply.started": "2024-07-18T02:12:02.337317Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "from train_eval import train, init_network\n",
    "from importlib import import_module\n",
    "import argparse\n",
    "# from utils import build_dataset, build_iterator, get_time_dif\n",
    "\n",
    "\n",
    "dataset = 'THUCNews'  # 数据集\n",
    "\n",
    "model_name = 'bert'  # bert\n",
    "x = import_module('models.' + model_name)\n",
    "config = x.Config(dataset)\n",
    "# np.random.seed(1)\n",
    "# torch.manual_seed(1)\n",
    "# torch.cuda.manual_seed_all(1)\n",
    "# torch.backends.cudnn.deterministic = True  # 保证每次结果一样\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8b352ec-bc21-40c1-9864-6046fe6b8e5e",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2024-07-18T02:21:37.192242Z",
     "iopub.status.busy": "2024-07-18T02:21:37.191583Z",
     "iopub.status.idle": "2024-07-18T02:21:37.195529Z",
     "shell.execute_reply": "2024-07-18T02:21:37.194984Z",
     "shell.execute_reply.started": "2024-07-18T02:21:37.192190Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['中', '华', '女', '子', '学', '院', 'self', '-', '：', '本', '科', '层', '次', '仅', '1', '专', '业', '招', '男', '生']\n",
      "[704, 1290, 1957, 2094, 2110, 7368, 12178, 118, 8038, 3315, 4906, 2231, 3613, 788, 122, 683, 689, 2875, 4511, 4495]\n"
     ]
    }
   ],
   "source": [
    "token = config.tokenizer.tokenize('中华女子学院：本科层次仅1专业招男生')\n",
    "print(token)\n",
    "print(config.tokenizer.convert_tokens_to_ids(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d92c86d-0518-4873-8ef3-b014e623c083",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-18T02:12:55.282844Z",
     "iopub.status.busy": "2024-07-18T02:12:55.282490Z",
     "iopub.status.idle": "2024-07-18T02:12:55.286322Z",
     "shell.execute_reply": "2024-07-18T02:12:55.285683Z",
     "shell.execute_reply.started": "2024-07-18T02:12:55.282825Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([101, 704, 1290, 1957, 2094, 2110, 7368, 8038, 3315, 4906, 2231, 3613, 788, 122, 683, 689, 2875, 4511, 4495, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 3, 19, [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a56783d9-d734-48f5-b31e-4c19d83d61c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T04:24:17.468342Z",
     "iopub.status.busy": "2024-07-18T04:24:17.467991Z",
     "iopub.status.idle": "2024-07-18T04:24:18.374945Z",
     "shell.execute_reply": "2024-07-18T04:24:18.374269Z",
     "shell.execute_reply.started": "2024-07-18T04:24:17.468322Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "from train_eval import train, init_network\n",
    "from importlib import import_module\n",
    "import argparse\n",
    "# from utils import build_dataset, build_iterator, get_time_dif\n",
    "\n",
    "\n",
    "dataset = 'Newtrain_qinggan'  # 数据集\n",
    "\n",
    "model_name = 'bert'  # bert\n",
    "x = import_module('models.' + model_name)\n",
    "config = x.Config(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c6b4560-938c-49e3-80fe-8a2458755800",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-18T04:24:36.633457Z",
     "iopub.status.busy": "2024-07-18T04:24:36.633027Z",
     "iopub.status.idle": "2024-07-18T04:24:39.285450Z",
     "shell.execute_reply": "2024-07-18T04:24:39.284891Z",
     "shell.execute_reply.started": "2024-07-18T04:24:36.633434Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8000it [00:02, 3772.36it/s]\n",
      "2653it [00:00, 5344.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17360 2653\n",
      "defaultdict(<class 'int'>, {1: 5660, 2: 5720, 0: 5980})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import random\n",
    "from datetime import timedelta\n",
    "\n",
    "PAD, CLS = '[PAD]', '[CLS]'  # padding符号, bert中综合信息符号\n",
    "class_list = ['价格', '配置', '操控', '舒适性', '油耗', '动力', '内饰', '安全性', '空间', '外观']\n",
    "\n",
    "def build_dataset_bert(config):\n",
    "\n",
    "    def load_dataset(path, pad_size=32):\n",
    "        contents = []\n",
    "        with open(path, 'r', encoding='UTF-8') as f:\n",
    "            for line in tqdm(f):\n",
    "                lin = line.strip()\n",
    "                if not lin:\n",
    "                    continue\n",
    "                tmp_data = lin.split('\\t')\n",
    "                content = tmp_data[0].replace(\" \", \"\")\n",
    "                # content, label = lin.split('\\t')\n",
    "                token = config.tokenizer.tokenize(content)\n",
    "                zhuti_label = [class_list.index(t.split('#')[0]) for t in tmp_data[1:]]\n",
    "\n",
    "                cal_qinggan = sum([int(t.split('#')[1]) for t in tmp_data[1:]])\n",
    "                qinggan_label = 1\n",
    "                if cal_qinggan > 0:\n",
    "                    qinggan_label = 2\n",
    "                elif cal_qinggan < 0:\n",
    "                    qinggan_label = 0\n",
    "\n",
    "                token = [CLS] + token\n",
    "                seq_len = len(token)\n",
    "                mask = []\n",
    "                token_ids = config.tokenizer.convert_tokens_to_ids(token)\n",
    "\n",
    "                if pad_size:\n",
    "                    if len(token) < pad_size:\n",
    "                        mask = [1] * len(token_ids) + [0] * (pad_size - len(token))\n",
    "                        token_ids += ([0] * (pad_size - len(token)))\n",
    "                    else:\n",
    "                        mask = [1] * pad_size\n",
    "                        token_ids = token_ids[:pad_size]\n",
    "                        seq_len = pad_size\n",
    "                contents.append((token_ids, zhuti_label, int(qinggan_label), seq_len, mask))\n",
    "                if 'test' not in path:\n",
    "                    # 过采样\n",
    "                    if qinggan_label in [0, 2]:\n",
    "                        for copyy in range(4):\n",
    "                             contents.append((token_ids, zhuti_label, int(qinggan_label), seq_len, mask))\n",
    "                    # 欠采样\n",
    "                    # if qinggan_label not in [0, 2]:\n",
    "                    #     if random.random() > 0.2:\n",
    "                    #         contents.pop()\n",
    "        return contents\n",
    "    train = load_dataset(config.train_path, config.pad_size)\n",
    "    test = load_dataset(config.test_path, config.pad_size)\n",
    "    random.shuffle(train)\n",
    "    random.shuffle(test)\n",
    "    return train, test\n",
    "\n",
    "\n",
    "train, test = build_dataset_bert(config)\n",
    "print(len(train), len(test))\n",
    "from collections import defaultdict\n",
    "c = defaultdict(int)\n",
    "for i in train:\n",
    "    c[i[2]] += 1\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00354f0a-0493-4c74-bb7e-2959c75b1e51",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-18T04:25:06.008278Z",
     "iopub.status.busy": "2024-07-18T04:25:06.007567Z",
     "iopub.status.idle": "2024-07-18T04:25:06.018771Z",
     "shell.execute_reply": "2024-07-18T04:25:06.018155Z",
     "shell.execute_reply.started": "2024-07-18T04:25:06.008166Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[ 101,  671, 3635,  ..., 3221, 7478, 6206],\n",
      "        [ 101, 5543, 2940,  ..., 4157, 6586,    0],\n",
      "        [ 101, 6756, 6716,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 5917, 5905,  ..., 3419, 1377,  809],\n",
      "        [ 101, 2419, 4669,  ...,    0,    0,    0],\n",
      "        [ 101, 2769, 4638,  ..., 1772,  794,  129]]), tensor([32, 31, 21, 32, 30, 31, 20, 14, 32, 12, 15, 22, 32, 32, 20, 32, 32, 32,\n",
      "        27, 26, 32, 32, 23, 32, 32, 32, 32, 32, 13, 32, 32, 32, 21, 32, 27, 32,\n",
      "        32, 30, 26, 20, 32, 32, 32, 32, 28, 32, 14, 32, 32, 21, 32, 17, 32, 13,\n",
      "        19, 32, 32, 13, 24, 15, 25, 32, 32, 32, 32, 32, 17, 32, 32, 32, 32, 14,\n",
      "        21, 30, 32, 32, 17, 32, 32, 32, 32, 32, 32, 32, 27, 32, 32, 32, 21, 32,\n",
      "        13, 32, 27, 32, 19, 32, 32, 32, 32, 25, 32, 32, 32, 32, 32, 32, 32, 32,\n",
      "        32, 12, 32, 32, 32, 18, 32, 30, 16, 20, 32, 32, 32, 32, 32, 32, 32, 32,\n",
      "        16, 32]), tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from utils import build_iterator_bert\n",
    "train_iter = build_iterator_bert(train, config)\n",
    "test_iter = build_iterator_bert(test, config)\n",
    "\n",
    "print(train_iter.__next__()[0])\n",
    "# model = x.Model(config).to(config.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43f8ac99-3764-40e0-b6ea-47da36ccdebd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T04:25:08.037714Z",
     "iopub.status.busy": "2024-07-18T04:25:08.037349Z",
     "iopub.status.idle": "2024-07-18T04:25:14.210120Z",
     "shell.execute_reply": "2024-07-18T04:25:14.209241Z",
     "shell.execute_reply.started": "2024-07-18T04:25:08.037694Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = x.Model(config).to(config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f3a3869-f5ce-49d7-bcc1-ff4e3d0ca157",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2024-07-18T04:25:41.337869Z",
     "iopub.status.busy": "2024-07-18T04:25:41.337301Z",
     "iopub.status.idle": "2024-07-18T04:32:20.357916Z",
     "shell.execute_reply": "2024-07-18T04:32:20.357142Z",
     "shell.execute_reply.started": "2024-07-18T04:25:41.337849Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/workspace/Bertchinese/pytorch_pretrained/optimization.py:275: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1055.)\n",
      "  next_m.mul_(beta1).add_(1 - beta1, grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:      0,  Train Loss:   0.6,  Train Acc: 32.03%,  Val Loss:  0.44,  Val Acc: 43.42%,  Time: 0:06:39 *improve\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Newtrain_qinggan/log/bert/train_dict.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13026/3668787725.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_total\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m \u001b[0mtrain_qinggan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_13026/3668787725.py\u001b[0m in \u001b[0;36mtrain_qinggan\u001b[0;34m(config, model, train_iter, test_iter)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mtest_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dev_recall'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_r\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                 \u001b[0mtest_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dev_f1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_f1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/train_dict.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m                     \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# indent参数使输出更加易读\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/test_dict.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Newtrain_qinggan/log/bert/train_dict.json'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn import metrics\n",
    "import time\n",
    "from utils import get_time_dif\n",
    "from pytorch_pretrained.optimization import BertAdam\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn import metrics\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from utils import get_time_dif\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "config.num_epochs = 50\n",
    "config.log_path = dataset + '/log/' + config.model_name\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    参考 https://github.com/lonePatient/TorchBlocks\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gamma=2.0, alpha=1, epsilon=1.e-9, device=None):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        if isinstance(alpha, list):\n",
    "            self.alpha = torch.Tensor(alpha).to(config.device)\n",
    "        else:\n",
    "            self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input: model's output, shape of [batch_size, num_cls]\n",
    "            target: ground truth labels, shape of [batch_size]\n",
    "        Returns:\n",
    "            shape of [batch_size]\n",
    "        \"\"\"\n",
    "        num_labels = input.size(-1)\n",
    "        idx = target.view(-1, 1).long()\n",
    "        one_hot_key = torch.zeros(idx.size(0), num_labels, dtype=torch.float32, device=idx.device)\n",
    "        one_hot_key = one_hot_key.scatter_(1, idx, 1)\n",
    "        one_hot_key[:, 0] = 0  # ignore 0 index.\n",
    "        logits = torch.softmax(input, dim=-1)\n",
    "        # print(one_hot_key)\n",
    "        # print(sdf)\n",
    "        loss = -self.alpha * one_hot_key * torch.pow((1 - logits), self.gamma) * (logits + self.epsilon).log()\n",
    "        loss = loss.sum(1)\n",
    "        return loss.mean()\n",
    "\n",
    "def train_qinggan(config, model, train_iter, test_iter):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "    optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                         lr=config.learning_rate,\n",
    "                         warmup=0.05,\n",
    "                         t_total=len(train_iter) * config.num_epochs)\n",
    "    total_batch = 0  # 记录进行到多少batch\n",
    "    dev_best_loss = float('inf')\n",
    "    train_dict = defaultdict(list)\n",
    "    test_dict = defaultdict(list)\n",
    "    loss_fn = FocalLoss(alpha=[1, 1, 1], device = config.device)\n",
    "    last_improve = 0  # 记录上次验证集loss下降的batch数\n",
    "    flag = False  # 记录是否很久没有效果提升\n",
    "    model.train()\n",
    "    for epoch in range(config.num_epochs):\n",
    "        print('Epoch [{}/{}]'.format(epoch + 1, config.num_epochs))\n",
    "        for i, (trains, qinggan_label, zhuti_label) in enumerate(train_iter):\n",
    "            outputs = model(trains)\n",
    "            model.zero_grad()\n",
    "            # loss = F.cross_entropy(outputs, labels)\n",
    "            qinggan_label = qinggan_label.to(config.device)\n",
    "            # loss = FocalLoss(alpha=[8000 / 1196, 8000/5660, 8000/1144])\n",
    "            # loss = loss_fn(outputs, qinggan_label)\n",
    "            loss = F.cross_entropy(outputs, qinggan_label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if total_batch % 1 == 0:\n",
    "                # 每多少轮输出在训练集和验证集上的效果\n",
    "                true = qinggan_label.data.cpu()\n",
    "                predic = torch.max(outputs.data, 1)[1].cpu()\n",
    "                train_acc = metrics.accuracy_score(true, predic)\n",
    "                train_precision = metrics.precision_score(true, predic, average='macro', zero_division=0)\n",
    "                train_recall = metrics.recall_score(true, predic, average='macro', zero_division=0)\n",
    "                train_f1 = 2 * (train_precision * train_recall) / (train_precision + train_recall + 1e-5)\n",
    "                # dev_acc, dev_loss = evaluate_qinggan(config, model, dev_iter)\n",
    "                dev_acc, dev_loss, dev_p, dev_r, dev_f1 = evaluate_qinggan(config, model, test_iter)\n",
    "                if dev_loss < dev_best_loss:\n",
    "                    dev_best_loss = dev_loss\n",
    "                    torch.save(model.state_dict(), config.save_path)\n",
    "                    improve = '*improve'\n",
    "                    last_improve = total_batch\n",
    "                else:\n",
    "                    improve = ''\n",
    "                time_dif = get_time_dif(start_time)\n",
    "                msg = 'Iter: {0:>6},  Train Loss: {1:>5.2},  Train Acc: {2:>6.2%},  Val Loss: {3:>5.2},  Val Acc: {4:>6.2%},  Time: {5} {6}'\n",
    "                print(msg.format(total_batch, loss.item(), train_acc, dev_loss, dev_acc, time_dif, improve))\n",
    "                train_dict['train_loss'].append(loss.item())   \n",
    "                train_dict['train_acc'].append(train_acc)\n",
    "                train_dict['train_precision'].append(train_precision)\n",
    "                train_dict['train_recall'].append(train_recall)\n",
    "                train_dict['train_f1'].append(train_f1)\n",
    "                test_dict['dev_acc'].append(dev_acc)\n",
    "                test_dict['dev_loss'].append(dev_loss.item())\n",
    "                test_dict['dev_precision'].append(dev_p)\n",
    "                test_dict['dev_recall'].append(dev_r)\n",
    "                test_dict['dev_f1'].append(dev_f1)\n",
    "                with open(config.log_path + '/train_dict.json', 'w') as json_file:\n",
    "                    json.dump(train_dict, json_file, indent=4)  # indent参数使输出更加易读\n",
    "                with open(config.log_path + '/test_dict.json', 'w') as json_file:\n",
    "                    json.dump(test_dict, json_file, indent=4)  # indent参数使输出更加易读\n",
    "                model.train()\n",
    "            total_batch += 1\n",
    "            if total_batch - last_improve > config.require_improvement:\n",
    "                # 验证集loss超过1000batch没下降，结束训练\n",
    "                print(\"No optimization for a long time, auto-stopping...\")\n",
    "                flag = True\n",
    "                break\n",
    "        if flag:\n",
    "            break\n",
    "\n",
    "\n",
    "def evaluate_qinggan(config, model, data_iter, test=False):\n",
    "    model.eval()\n",
    "    loss_total = 0\n",
    "    predict_all = np.array([], dtype=int)\n",
    "    labels_all = np.array([], dtype=int)\n",
    "    loss_fn = FocalLoss(alpha=[1, 1, 1],  device = config.device)\n",
    "    with torch.no_grad():\n",
    "        for texts, qinggan_label, zhuti_label in data_iter:\n",
    "            outputs = model(texts)\n",
    "\n",
    "            qinggan_label = qinggan_label.to(config.device)\n",
    "            loss = F.cross_entropy(outputs, qinggan_label)\n",
    "            # loss = loss_fn(outputs, qinggan_label)\n",
    "            loss_total += loss\n",
    "            labels = qinggan_label.data.cpu().numpy()\n",
    "            predic = torch.max(outputs.data, 1)[1].cpu().numpy()\n",
    "            labels_all = np.append(labels_all, labels)\n",
    "            predict_all = np.append(predict_all, predic)\n",
    "\n",
    "    acc = metrics.accuracy_score(labels_all, predict_all)\n",
    "    p = metrics.precision_score(labels_all, predict_all, average='macro', zero_division=0)\n",
    "    r = metrics.recall_score(labels_all, predict_all, average='macro', zero_division=0)\n",
    "    f1 =  2 * (p * r) / (p + r + 1e-5)\n",
    "\n",
    "    return acc, loss_total / len(data_iter), p, r, f1\n",
    "\n",
    "train_qinggan(config, model, train_iter, test_iter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
